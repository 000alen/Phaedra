{"id": "ed5dd699-ae06-4a07-90eb-12b829776f19", "name": "recursing_hoover", "file": "attention.pdf", "pages": [{"id": "6935d71f-a485-4d54-bf99-735f8c572cfe", "cells": [{"id": "5fe5157c-0543-45d5-b39d-0e41dfa4e7bc", "content": "Our model achieves 28.4 bleu on the wmt 2014 english- to-german translation task, improving over the existing best results. We propose a new simple network architecture, the transformer, entirely. Tensor2.llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and z = (z,...,z ). The decoder maps an input sequence of symbol representations (x, [...],x ) to an output", "data": {}}], "data": {"source": "Attention Is All You Need\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nentirely. Experiments on two machine translation tasks show these models to\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nRecurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetworks\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous\narchitectures[31,21,13].\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\n", "number": 1}}, {"id": "d3c11a14-d85c-44e3-a99f-e092fc8c35e5", "cells": [{"id": "ae06f488-f860-4b0b-8cfe-c12506fb859f", "content": "Each layer has two identical layers, followed by a layer normalization. The output of each sub-layer is d =d /h=64, and the dimensionality of input and output is determined by the kernel size 1 of the decoder stack. 3.2 attention 3.2.1 position-wisefeed-forwardnetworks connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This from layer to layer.", "data": {}}], "data": {"source": "signi\ufb01cantimprovementsincomputationalef\ufb01ciencythroughfactorizationtricks[18]andconditional\ncomputation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\ntheinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms\n[20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ntextualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19].\nlanguagemodelingtasks[28].\nself-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8].\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n[9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n", "number": 2}}, {"id": "4830c99b-8bef-47b0-886f-40dc6dda3bb6", "cells": [], "data": {"source": "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n3.2 Attention\n3.2.1 ScaledDot-ProductAttention\n", "number": 3}}, {"id": "e9bd136f-1b9d-43dd-90a9-bb12a8f914ca", "cells": [], "data": {"source": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\nd\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\n3.2.2 Multi-HeadAttention\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\n", "number": 4}}, {"id": "dd3df367-89be-4332-8db6-94c149bc0bca", "cells": [{"id": "d387e32f-6142-4a29-8788-3bd99e9080c3", "content": "Wealsocompare executed operations, whereas a recurrent layer requires o(n) sequential operations. N is smaller than the representation dimensionality d, which is most often the case with [31]andbyte-pair[25]representations. Wevariedthelearning 5.4 regularization of embeddingsandsoftmax mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities[24].", "data": {}}], "data": {"source": "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\n3.2.3 ApplicationsofAttentioninourModel\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31,2,8].\n3.3 Position-wiseFeed-ForwardNetworks\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\nd =2048.\n3.4 EmbeddingsandSoftmax\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\nlineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d .\n3.5 PositionalEncoding\n", "number": 5}}, {"id": "92313bd0-8ee8-442c-80ac-47a24a6072d4", "cells": [], "data": {"source": "learnedand\ufb01xed[8].\nWealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\nandoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\nlength n is smaller than the representation dimensionality d, which is most often the case with\n[31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving\n", "number": 6}}, {"id": "aa0038a4-8ad6-40fe-b0b9-5c474596a2d2", "cells": [], "data": {"source": "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n5.1 TrainingDataandBatching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nvocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\n5.2 HardwareandSchedule\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\n5.3 Optimizer\nWeusedtheAdamoptimizer[17]with\u03b2 =0.9,\u03b2 =0.98and(cid:15)=10\u22129. Wevariedthelearning\n5.4 Regularization\n", "number": 7}}, {"id": "a5531f48-eb28-4277-a142-a3e22b7a1a5f", "cells": [{"id": "17f69f51-5ff2-4d84-bd2e-701ba9da8573", "content": "We averagedthelast20checkpoints, which werewritten at10-minuteintervals. A more sophisticated compatibility on recurrent or convolutional layers is available at https://github.com/modelsandplantoapplythemtoothertasks. The code we used to train and evaluate our models was available on both wmt 2014 english-to-german and gmc2014. I'm not sure if this is the case.", "data": {}}], "data": {"source": "ByteNet[15] 23.75\nDeep-Att+PosUnk[32] 39.2 1.0\u00b71020\nGNMT+RL[31] 24.6 39.92 2.3\u00b71019 1.4\u00b71020\nConvS2S[8] 25.16 40.46 9.6\u00b71018 1.5\u00b71020\nMoE[26] 26.03 40.56 2.0\u00b71019 1.2\u00b71020\nDeep-Att+PosUnkEnsemble[32] 40.4 8.0\u00b71020\nGNMT+RLEnsemble[31] 26.30 41.16 1.8\u00b71020 1.1\u00b71021\nConvS2SEnsemble[8] 26.36 41.29 7.7\u00b71019 1.2\u00b71021\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This\n6.1 MachineTranslation\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\ninferencetoinputlength+50,butterminateearlywhenpossible[31].\n6.2 ModelVariations\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\n", "number": 8}}, {"id": "abc36cee-2d14-4e13-9117-5205c53f0738", "cells": [], "data": {"source": "(A)\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nWeareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\nThe code we used to train and evaluate our models is available at https://github.com/\n", "number": 9}}, {"id": "71a49711-275f-43b9-8c11-edaf7b920633", "cells": [{"id": "00434532-e282-43cd-8b50-960add27ac0e", "content": "Arxiv preprint [10] junyoungchung, aglarg\u00fcl\u00e7ehre,kyunghyuncho,andyoshuabengio. Deep residual learning for im- age recognition. Xception: deep learning with depthwise separable convolutions. In international conference on computer vision and pattern [11] sepp-hochreiter,yokohama,paolofrasconi, and j\u00fcrgen schmid", "data": {}}, {"id": "37c74a63-3e7b-47e0-93cf-171fdd356423", "content": "Arxivpreprint [19] zhouhan lin, minwei feng, cicero nogueira dos santos, mo yu. A structured self-attentive sentence embedding. Ankurparikh,oscart\u00e4ckstr\u00f6m,dipanjandas,andjakobuszkoreit. Adecomposableattention. [21] noamshazeer,azaliamirhoseini,krzystofm", "data": {}}], "data": {"source": "[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n[7] JunyoungChung,\u00c7aglarG\u00fcl\u00e7ehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\n[8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n[11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJ\u00fcrgenSchmidhuber. Gradient\ufb02owin\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n[13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\n[14] \u0141ukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\n[15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\n[16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\n[17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\n[18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n[20] SamyBengio\u0141ukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural\n", "number": 10}}, {"id": "c4ec7b7e-7ab8-4550-b4a9-81cff9dddb6a", "cells": [{"id": "c306bf0a-dbfc-432b-914e-95445ec23694", "content": "End-to-end memory [29] ilyasutskever,oriolvinyals, andquocvvle. Deep recurrent models with deep-resistance memory with a wide range of features. Yonghui wu, mike schuster, zhifeng chen, mohammad norouzi, wolfgang, jie zhou, ying cao and xuguang wang, peng li.", "data": {}}], "data": {"source": "[21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\n[22] AnkurParikh,OscarT\u00e4ckstr\u00f6m,DipanjanDas,andJakobUszkoreit. Adecomposableattention\n[23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\n[24] O\ufb01rPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\n[25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\n[26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\n[27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\n[29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\n[30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n", "number": 11}}]}